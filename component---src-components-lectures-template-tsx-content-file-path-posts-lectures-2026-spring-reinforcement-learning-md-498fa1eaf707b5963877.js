"use strict";(self.webpackChunkp4c_lab=self.webpackChunkp4c_lab||[]).push([[8458],{2144:function(e,n,t){t.r(n),t.d(n,{Head:function(){return d},default:function(){return E}});var a=t(8453),l=t(6540);function r(e){const n=Object.assign({h1:"h1",a:"a",span:"span",h2:"h2",ul:"ul",li:"li",h3:"h3",hr:"hr"},(0,a.RP)(),e.components);return l.createElement(l.Fragment,null,l.createElement(n.h1,{id:"instruction"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#instruction"},l.createElement(n.span,{className:"icon icon-link"})),"Instruction"),"\n",l.createElement(n.h2,{id:"course-staff"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#course-staff"},l.createElement(n.span,{className:"icon icon-link"})),"Course Staff"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"Lecturer: Woohyeok Choi","\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"Office: #407, College of Engineering #6"),"\n",l.createElement(n.li,null,"Mail: ",l.createElement(n.a,{href:"mailto:woohyeok.choi@kangwon.ac.kr"},"woohyeok.choi@kangwon.ac.kr")),"\n"),"\n"),"\n",l.createElement(n.li,null,"Teaching Assistant: TBA"),"\n"),"\n",l.createElement(n.h2,{id:"time--location"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#time--location"},l.createElement(n.span,{className:"icon icon-link"})),"Time & Location"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"Mon./Thu. 09:00 - 10:15, #600, College of Engineering #6"),"\n"),"\n",l.createElement(n.h2,{id:"office-hours"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#office-hours"},l.createElement(n.span,{className:"icon icon-link"})),"Office Hours"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"Tue. 13:00 - 15:00"),"\n"),"\n",l.createElement(n.h2,{id:"textbook"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#textbook"},l.createElement(n.span,{className:"icon icon-link"})),"Textbook"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"[Ri20] Reinforcement Learning: An Introduction, 2nd Ed., Richard S. Sutton and Andrew G. Barto, The MIT Press."),"\n"),"\n",l.createElement(n.h2,{id:"prerequisite"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#prerequisite"},l.createElement(n.span,{className:"icon icon-link"})),"Prerequisite"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"Python Programming"),"\n"),"\n",l.createElement(n.h2,{id:"grading-policy"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#grading-policy"},l.createElement(n.span,{className:"icon icon-link"})),"Grading Policy"),"\n",l.createElement(n.h3,{id:"reinforcement-learning-competitions-90"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#reinforcement-learning-competitions-90"},l.createElement(n.span,{className:"icon icon-link"})),"Reinforcement Learning Competitions (90%)"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"(5%) Competition Round 0: TBA"),"\n",l.createElement(n.li,null,"(15%) Competition Round 1: TBA"),"\n",l.createElement(n.li,null,"(20%) Competition Round 2: TBA"),"\n",l.createElement(n.li,null,"(20%) Competition Round 3: TBA"),"\n",l.createElement(n.li,null,"(30%) Competition Round 4: TBA"),"\n"),"\n",l.createElement(n.h3,{id:"attendance-10"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#attendance-10"},l.createElement(n.span,{className:"icon icon-link"})),"Attendance (10%)"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"1% of credit is deducted for each absence or each 3-lateness"),"\n",l.createElement(n.li,null,"At least 11-Absence = F grade"),"\n"),"\n",l.createElement(n.h1,{id:"schedule"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#schedule"},l.createElement(n.span,{className:"icon icon-link"})),"Schedule"),"\n",l.createElement(n.h2,{id:"week-01"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#week-01"},l.createElement(n.span,{className:"icon icon-link"})),"Week 01"),"\n",l.createElement(n.h3,{id:"march-05--overview--logistics"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#march-05--overview--logistics"},l.createElement(n.span,{className:"icon icon-link"})),"March 05 — Overview & Logistics"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lecture")),"\n"),"\n",l.createElement(n.h3,{id:"march-09--basic-math"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#march-09--basic-math"},l.createElement(n.span,{className:"icon icon-link"})),"March 09 — Basic Math"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lecture")),"\n"),"\n",l.createElement(n.hr),"\n",l.createElement(n.h2,{id:"week-02"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#week-02"},l.createElement(n.span,{className:"icon icon-link"})),"Week 02"),"\n",l.createElement(n.h3,{id:"march-12--introduction-to-reinforcement-learning"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#march-12--introduction-to-reinforcement-learning"},l.createElement(n.span,{className:"icon icon-link"})),"March 12 — Introduction to Reinforcement Learning"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lecture")),"\n",l.createElement(n.li,null,"Reference","\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"[Ri20] Chap. 1"),"\n"),"\n"),"\n"),"\n",l.createElement(n.h3,{id:"march-16--multi-armed-bandits"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#march-16--multi-armed-bandits"},l.createElement(n.span,{className:"icon icon-link"})),"March 16 — Multi-Armed Bandits"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lecture")),"\n",l.createElement(n.li,null,"Reference","\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"[Ri20] Chap. 2"),"\n"),"\n"),"\n"),"\n",l.createElement(n.hr),"\n",l.createElement(n.h2,{id:"week-03"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#week-03"},l.createElement(n.span,{className:"icon icon-link"})),"Week 03"),"\n",l.createElement(n.h3,{id:"march-19--markov-process"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#march-19--markov-process"},l.createElement(n.span,{className:"icon icon-link"})),"March 19 — Markov Process"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lecture")),"\n",l.createElement(n.li,null,"Reference","\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"[Ri20] Chap. 3"),"\n"),"\n"),"\n"),"\n",l.createElement(n.h3,{id:"march-23--dynamic-programming"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#march-23--dynamic-programming"},l.createElement(n.span,{className:"icon icon-link"})),"March 23 — Dynamic Programming"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lecture")),"\n",l.createElement(n.li,null,"Reference","\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"[Ri20] Chap. 4"),"\n"),"\n"),"\n"),"\n",l.createElement(n.hr),"\n",l.createElement(n.h2,{id:"week-04"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#week-04"},l.createElement(n.span,{className:"icon icon-link"})),"Week 04"),"\n",l.createElement(n.h3,{id:"march-26--tutorial-on-gymnasium"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#march-26--tutorial-on-gymnasium"},l.createElement(n.span,{className:"icon icon-link"})),"March 26 — Tutorial on Gymnasium"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lecture")),"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"(Announce) Competition Round 0: Grid Crossing!"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"Due: April 06"),"\n"),"\n"),"\n",l.createElement(n.li,null,"Readings","\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:"https://gymnasium.farama.org/",target:"_blank",rel:"nofollow noopener noreferrer"},"OpenAI Gymnasium")),"\n"),"\n"),"\n"),"\n",l.createElement(n.h3,{id:"march-30--monte-carlo-methods-on-policy-methods"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#march-30--monte-carlo-methods-on-policy-methods"},l.createElement(n.span,{className:"icon icon-link"})),"March 30 — Monte-Carlo Methods: On-Policy Methods"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lecture")),"\n",l.createElement(n.li,null,"Reference","\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"[Ri20] Chap. 5"),"\n"),"\n"),"\n"),"\n",l.createElement(n.hr),"\n",l.createElement(n.h2,{id:"week-05"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#week-05"},l.createElement(n.span,{className:"icon icon-link"})),"Week 05"),"\n",l.createElement(n.h3,{id:"april-02--monte-carlo-methods-off-policy-methods"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#april-02--monte-carlo-methods-off-policy-methods"},l.createElement(n.span,{className:"icon icon-link"})),"April 02 — Monte-Carlo Methods: Off-Policy Methods"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lecture")),"\n",l.createElement(n.li,null,"Reference","\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"[Ri20] Chap. 5"),"\n"),"\n"),"\n"),"\n",l.createElement(n.h3,{id:"april-06--competition-round-0-tba"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#april-06--competition-round-0-tba"},l.createElement(n.span,{className:"icon icon-link"})),"April 06 — Competition Round 0: TBA"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Leaderboard")),"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"(Announce) Competition Round 1: TBA"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"Due: April 23"),"\n"),"\n"),"\n"),"\n",l.createElement(n.hr),"\n",l.createElement(n.h2,{id:"week-06"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#week-06"},l.createElement(n.span,{className:"icon icon-link"})),"Week 06"),"\n",l.createElement(n.h3,{id:"april-09--temporal-difference-learning"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#april-09--temporal-difference-learning"},l.createElement(n.span,{className:"icon icon-link"})),"April 09 — Temporal Difference Learning"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lecture")),"\n",l.createElement(n.li,null,"Reference","\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"[Ri20] Chap. 6"),"\n"),"\n"),"\n"),"\n",l.createElement(n.h3,{id:"april-13--n-step-bootstrapping"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#april-13--n-step-bootstrapping"},l.createElement(n.span,{className:"icon icon-link"})),"April 13 — n-Step Bootstrapping"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lecture")),"\n",l.createElement(n.li,null,"Reference","\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"[Ri20] Chap. 7"),"\n"),"\n"),"\n"),"\n",l.createElement(n.hr),"\n",l.createElement(n.h2,{id:"week-07"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#week-07"},l.createElement(n.span,{className:"icon icon-link"})),"Week 07"),"\n",l.createElement(n.h3,{id:"april-16--planning--learning"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#april-16--planning--learning"},l.createElement(n.span,{className:"icon icon-link"})),"April 16 — Planning & Learning"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lecture")),"\n",l.createElement(n.li,null,"Reference","\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"[Ri20] Chap. 8"),"\n"),"\n"),"\n"),"\n",l.createElement(n.h3,{id:"april-20--linear-function-approximation"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#april-20--linear-function-approximation"},l.createElement(n.span,{className:"icon icon-link"})),"April 20 — Linear Function Approximation"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lecture")),"\n",l.createElement(n.li,null,"Reference","\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"[Ri20] Chap. 9 - 10"),"\n"),"\n"),"\n"),"\n",l.createElement(n.hr),"\n",l.createElement(n.h2,{id:"week-08"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#week-08"},l.createElement(n.span,{className:"icon icon-link"})),"Week 08"),"\n",l.createElement(n.h3,{id:"april-23--competition-round-1-tba"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#april-23--competition-round-1-tba"},l.createElement(n.span,{className:"icon icon-link"})),"April 23 — Competition Round 1: TBA"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Leaderboard")),"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"(Announce) Competition Round 2: TBA"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"Due: May 11"),"\n"),"\n"),"\n"),"\n",l.createElement(n.h3,{id:"april-27--nonlinear-function-approximation-deep-neural-network"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#april-27--nonlinear-function-approximation-deep-neural-network"},l.createElement(n.span,{className:"icon icon-link"})),"April 27 — Nonlinear Function Approximation: Deep Neural Network"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lecture")),"\n",l.createElement(n.li,null,"Reference","\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"[Ge23] Chap. 10, 11"),"\n",l.createElement(n.li,null,"[Ri20] Chap. 9 - 10"),"\n"),"\n"),"\n"),"\n",l.createElement(n.hr),"\n",l.createElement(n.h2,{id:"week-09"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#week-09"},l.createElement(n.span,{className:"icon icon-link"})),"Week 09"),"\n",l.createElement(n.h3,{id:"april-30--nonlinear-function-approximation-convolution-neural-network"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#april-30--nonlinear-function-approximation-convolution-neural-network"},l.createElement(n.span,{className:"icon icon-link"})),"April 30 — Nonlinear Function Approximation: Convolution Neural Network"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lecture")),"\n",l.createElement(n.li,null,"Reference","\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"[Ge23] Chap. 14"),"\n",l.createElement(n.li,null,"[Ri20] Chap. 9 - 10"),"\n"),"\n"),"\n"),"\n",l.createElement(n.h3,{id:"may-04--practice-on-function-approximation"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#may-04--practice-on-function-approximation"},l.createElement(n.span,{className:"icon icon-link"})),"May 04 — Practice on Function Approximation"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lectures")),"\n",l.createElement(n.li,null,"Reference","\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"[Ge23] Chap. 10, 11, 14"),"\n",l.createElement(n.li,null,"[Ri20] Chap. 9 - 10"),"\n"),"\n"),"\n"),"\n",l.createElement(n.hr),"\n",l.createElement(n.h2,{id:"week-10"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#week-10"},l.createElement(n.span,{className:"icon icon-link"})),"Week 10"),"\n",l.createElement(n.h3,{id:"may-07--deep-q-network"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#may-07--deep-q-network"},l.createElement(n.span,{className:"icon icon-link"})),"May 07 — Deep-Q Network"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lecture")),"\n",l.createElement(n.li,null,"Reference","\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"[Ri20] Chap. 11"),"\n",l.createElement(n.li,null,"Mnih, V., Kavukcuoglu, K., Silver, D. et al. “Human-level control through deep reinforcement learning”. Nature 518, 529–533 (2015). ",l.createElement(n.a,{href:"https://doi.org/10.1038/nature14236",target:"_blank",rel:"nofollow noopener noreferrer"},"https://doi.org/10.1038/nature14236")),"\n"),"\n"),"\n"),"\n",l.createElement(n.h3,{id:"may-11--competition-round-2-tba"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#may-11--competition-round-2-tba"},l.createElement(n.span,{className:"icon icon-link"})),"May 11 — Competition Round 2: TBA"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Leaderboard")),"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"(Announce) Competition Round 3: TBA"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"Due: June 18"),"\n"),"\n"),"\n"),"\n",l.createElement(n.hr),"\n",l.createElement(n.h2,{id:"week-11"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#week-11"},l.createElement(n.span,{className:"icon icon-link"})),"Week 11"),"\n",l.createElement(n.h3,{id:"may-14--policy-gradient-methods"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#may-14--policy-gradient-methods"},l.createElement(n.span,{className:"icon icon-link"})),"May 14 — Policy Gradient Methods"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lecture")),"\n",l.createElement(n.li,null,"Reference","\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"[Ri20] Chap. 13"),"\n"),"\n"),"\n"),"\n",l.createElement(n.h3,{id:"may-18--practice-on-policy-gradient-methods"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#may-18--practice-on-policy-gradient-methods"},l.createElement(n.span,{className:"icon icon-link"})),"May 18 — Practice on Policy Gradient Methods"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lecture")),"\n",l.createElement(n.li,null,"Reference","\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"[Ri20] Chap. 13"),"\n"),"\n"),"\n"),"\n",l.createElement(n.hr),"\n",l.createElement(n.h2,{id:"week-12"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#week-12"},l.createElement(n.span,{className:"icon icon-link"})),"Week 12"),"\n",l.createElement(n.h3,{id:"may-21--advanced-topics-variants-of-dqn--asynchronous-methods"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#may-21--advanced-topics-variants-of-dqn--asynchronous-methods"},l.createElement(n.span,{className:"icon icon-link"})),"May 21 — Advanced Topics: Variants of DQN / Asynchronous Methods"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lecture")),"\n",l.createElement(n.li,null,"Reference","\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,'Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D. & Kavukcuoglu, K.. "Asynchronous Methods for Deep Reinforcement Learning". Proceedings of The 33rd International Conference on Machine Learning. 48:1928-1937 (2016). ',l.createElement(n.a,{href:"https://proceedings.mlr.press/v48/mniha16.html",target:"_blank",rel:"nofollow noopener noreferrer"},"https://proceedings.mlr.press/v48/mniha16.html"),"."),"\n",l.createElement(n.li,null,'van Hasselt, H., Guez, A., & Silver, D. "Deep Reinforcement Learning with Double Q-Learning". Proceedings of the AAAI Conference on Artificial Intelligence, 30 (1) (2016). ',l.createElement(n.a,{href:"https://doi.org/10.1609/aaai.v30i1.10295",target:"_blank",rel:"nofollow noopener noreferrer"},"https://doi.org/10.1609/aaai.v30i1.10295")),"\n",l.createElement(n.li,null,'Schaul, T., Quan, J., Antonoglou, I., Silver, D. "Prioritized Experience Replay." arXiv preprint arXiv:1511.05952 (2015). ',l.createElement(n.a,{href:"https://arxiv.org/abs/1511.05952",target:"_blank",rel:"nofollow noopener noreferrer"},"https://arxiv.org/abs/1511.05952")),"\n",l.createElement(n.li,null,'Wang, Ziyu, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. "Dueling network architectures for deep reinforcement learning." In International conference on machine learning, pp. 1995-2003. PMLR, 2016. ',l.createElement(n.a,{href:"https://proceedings.mlr.press/v48/wangf16.html",target:"_blank",rel:"nofollow noopener noreferrer"},"https://proceedings.mlr.press/v48/wangf16.html")),"\n"),"\n"),"\n"),"\n",l.createElement(n.h3,{id:"may-25--buddhas-birthday"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#may-25--buddhas-birthday"},l.createElement(n.span,{className:"icon icon-link"})),"May 25 — Buddha's Birthday"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"No Class"),"\n"),"\n",l.createElement(n.hr),"\n",l.createElement(n.h2,{id:"week-13"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#week-13"},l.createElement(n.span,{className:"icon icon-link"})),"Week 13"),"\n",l.createElement(n.h3,{id:"may-28--competition-round-3-tba"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#may-28--competition-round-3-tba"},l.createElement(n.span,{className:"icon icon-link"})),"May 28 — Competition Round 3: TBA"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Leaderboard")),"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"(Announce) Competition Round 4: TBA"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"Due: June 15"),"\n"),"\n"),"\n"),"\n",l.createElement(n.h3,{id:"june-01--advanced-topics-deterministic-policy-gradient-methods"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#june-01--advanced-topics-deterministic-policy-gradient-methods"},l.createElement(n.span,{className:"icon icon-link"})),"June 01 — Advanced Topics: Deterministic Policy Gradient Methods"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lecture")),"\n",l.createElement(n.li,null,"Reference","\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,'Lillicrap, Timothy P., Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. "Continuous control with deep reinforcement learning." arXiv preprint arXiv:1509.02971 (2015). ',l.createElement(n.a,{href:"https://arxiv.org/abs/1509.02971",target:"_blank",rel:"nofollow noopener noreferrer"},"https://arxiv.org/abs/1509.02971")),"\n",l.createElement(n.li,null,'Dankwa, Stephen and Zheng, Wenfeng. "Twin-Delayed DDPG: A Deep Reinforcement Learning Technique to Model a Continuous Movement of an Intelligent Robot Agent". In Proceedings of the 3rd International Conference on Vision, Image and Signal Processing (ICVISP 2019). Association for Computing Machinery, New York, NY, USA, Article 66, 1–5. (2020) ',l.createElement(n.a,{href:"https://doi.org/10.1145/3387168.3387199",target:"_blank",rel:"nofollow noopener noreferrer"},"https://doi.org/10.1145/3387168.3387199")),"\n"),"\n"),"\n"),"\n",l.createElement(n.hr),"\n",l.createElement(n.h2,{id:"week-14"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#week-14"},l.createElement(n.span,{className:"icon icon-link"})),"Week 14"),"\n",l.createElement(n.h3,{id:"june-04--advanced-topics-entropy-regularized-methods"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#june-04--advanced-topics-entropy-regularized-methods"},l.createElement(n.span,{className:"icon icon-link"})),"June 04 — Advanced Topics: Entropy-Regularized Methods"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lecture")),"\n",l.createElement(n.li,null,"Reference","\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,'Haarnoja, Tuomas, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar. "Soft actor-critic algorithms and applications." arXiv preprint arXiv:1812.05905 (2018). ',l.createElement(n.a,{href:"https://arxiv.org/abs/1812.05905",target:"_blank",rel:"nofollow noopener noreferrer"},"https://arxiv.org/abs/1812.05905")),"\n"),"\n"),"\n"),"\n",l.createElement(n.h3,{id:"june-08--advanced-topics-trust-region-constraint-methods"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#june-08--advanced-topics-trust-region-constraint-methods"},l.createElement(n.span,{className:"icon icon-link"})),"June 08 — Advanced Topics: Trust Region Constraint Methods"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lecture")),"\n",l.createElement(n.li,null,"Reference","\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,'Schulman, John, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. "Trust region policy optimization." In International conference on machine learning, pp. 1889-1897. PMLR, 2015.'),"\n",l.createElement(n.li,null,'Schulman, John, et al. "Proximal policy optimization algorithms." arXiv preprint arXiv:1707.06347 (2017). ',l.createElement(n.a,{href:"https://arxiv.org/abs/1707.06347",target:"_blank",rel:"nofollow noopener noreferrer"},"https://arxiv.org/abs/1707.06347")),"\n"),"\n"),"\n"),"\n",l.createElement(n.h2,{id:"week-15"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#week-15"},l.createElement(n.span,{className:"icon icon-link"})),"Week 15"),"\n",l.createElement(n.h3,{id:"june-11--focus-on-rl-final-competition"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#june-11--focus-on-rl-final-competition"},l.createElement(n.span,{className:"icon icon-link"})),"June 11 — Focus on RL Final Competition"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,"No Class"),"\n"),"\n",l.createElement(n.h3,{id:"june-15--competition-round-4-tba"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#june-15--competition-round-4-tba"},l.createElement(n.span,{className:"icon icon-link"})),"June 15 — Competition Round 4: TBA"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Leaderboard")),"\n"),"\n",l.createElement(n.hr),"\n",l.createElement(n.h2,{id:"week-16"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#week-16"},l.createElement(n.span,{className:"icon icon-link"})),"Week 16"),"\n",l.createElement(n.h3,{id:"june-18--final-remark"},l.createElement(n.a,{"aria-hidden":"true",tabIndex:"-1",href:"#june-18--final-remark"},l.createElement(n.span,{className:"icon icon-link"})),"June 18 — Final Remark"),"\n",l.createElement(n.ul,null,"\n",l.createElement(n.li,null,l.createElement(n.a,{href:""},"Lecture")),"\n"),"\n",l.createElement(n.hr))}var i=function(e){void 0===e&&(e={});const{wrapper:n}=Object.assign({},(0,a.RP)(),e.components);return n?l.createElement(n,e,l.createElement(r,e)):r(e)},c=t(3841),o=t(1042),m=t(1094),u=t(9653);const s=e=>{var n,t,a,r,i,o;let{data:s,children:d}=e;const{lecture:E}=s;return l.createElement(c.A,{activeLink:"lectures"},l.createElement("div",null,l.createElement("div",{className:"flex flex-col text-base items-start font-serif space-y-2"},l.createElement(m.L9,{className:"capitalize"},(null==E||null===(n=E.frontmatter)||void 0===n?void 0:n.year)||""," ",(null==E||null===(t=E.frontmatter)||void 0===t?void 0:t.semester)||""),l.createElement(m.H2,{className:"font-sans"},(null==E||null===(a=E.frontmatter)||void 0===a?void 0:a.title)||""," ",(null==E||null===(r=E.frontmatter)||void 0===r?void 0:r.division)&&`(${(null==E||null===(i=E.frontmatter)||void 0===i?void 0:i.division)||""})`),l.createElement(m.bq,null,null==E||null===(o=E.frontmatter)||void 0===o?void 0:o.description)),l.createElement("hr",{className:"my-6"}),l.createElement(u.A,null,d)))},d=e=>{var n,t,a,r,i,c;let{data:m}=e;return l.createElement(o.A,{title:`Lecture - ${null===(n=m.lecture)||void 0===n||null===(t=n.frontmatter)||void 0===t?void 0:t.year} ${null===(a=m.lecture)||void 0===a||null===(r=a.frontmatter)||void 0===r?void 0:r.title}`,description:(null===(i=m.lecture)||void 0===i||null===(c=i.frontmatter)||void 0===c?void 0:c.description)||""})};function E(e){return l.createElement(s,e,l.createElement(i,e))}},8453:function(e,n,t){t.d(n,{RP:function(){return r},xA:function(){return c}});var a=t(6540);const l=a.createContext({});function r(e){const n=a.useContext(l);return a.useMemo(()=>"function"==typeof e?e(n):{...n,...e},[n,e])}const i={};function c({components:e,children:n,disableParentContext:t}){let c;return c=t?"function"==typeof e?e({}):e||i:r(e),a.createElement(l.Provider,{value:c},n)}},9653:function(e,n,t){var a=t(6540),l=t(8453),r=t(1094);n.A=e=>{let{children:n}=e;const t={h1:e=>a.createElement(r.H4,Object.assign({className:"mt-12 mb-1 first:mt-0"},e)),h2:e=>a.createElement(r.H5,Object.assign({className:"mt-4 mb-1"},e)),h3:e=>a.createElement(r.H6,Object.assign({className:"mt-2 mb-1"},e)),p:e=>a.createElement(r.P,Object.assign({className:"mt-0 mb-2"},e)),ul:e=>a.createElement("ul",Object.assign({className:"my-0 list-outside list-disc text-md text-gray-500 font-serif dark:text-gray-400"},e)),ol:e=>a.createElement("ol",Object.assign({className:"my-0 list-outside list-decimal text-md text-gray-500 font-serif dark:text-gray-400"},e)),li:e=>a.createElement("li",Object.assign({className:"my-0"},e)),a:e=>a.createElement(r.A,e),pre:e=>a.createElement("pre",Object.assign({className:"p-0"},e)),table:e=>a.createElement("table",Object.assign({className:"font-medium text-md font-serif"},e))};return a.createElement("article",{className:"max-w-full dark:format-invert format"},a.createElement(l.xA,{components:t},n))}}}]);
//# sourceMappingURL=component---src-components-lectures-template-tsx-content-file-path-posts-lectures-2026-spring-reinforcement-learning-md-498fa1eaf707b5963877.js.map