{"componentChunkName":"component---src-components-lectures-template-tsx-content-file-path-posts-lectures-2025-fall-ai-md","path":"/lectures/ac9c858d-dfae-52f4-a836-f149ad99c2d6/","result":{"data":{"lecture":{"frontmatter":{"code":"4471032","title":"Artificial Intelligence","year":"2025","semester":"fall","division":"2 Div.","description":"Reinforcement learning (RL) is one of the popular machine learning paradigms for solving sequential decision-making problems. In this paradigm, agents learn the optimal policies by repeatedly interacting with an environment to maximize (cumulative) rewards. This courses will cover the foundational concepts of RL, including state-action-reward pairs, the Markov decision process, and exploration versus exploitation. In addition, we will learn key RL algorithms, such as the Monte Carlo method, temporal difference learning, function approximation, and policy gradients. Furthermore, you will work on a small team project to implement an RL agent to solve problems with different difficulties, from simple to complex ones."},"tableOfContents":{"items":[{"url":"#instruction","title":"Instruction","items":[{"url":"#course-staff","title":"Course Staff"},{"url":"#time--location","title":"Time & Location"},{"url":"#office-hours","title":"Office Hours"},{"url":"#textbook","title":"Textbook"},{"url":"#prerequisite","title":"Prerequisite"},{"url":"#grading-policy","title":"Grading Policy","items":[{"url":"#reinforcement-learning-competitions-90","title":"Reinforcement Learning Competitions (90%)"},{"url":"#attendance-10","title":"Attendance (10%)"}]}]},{"url":"#schedule","title":"Schedule","items":[{"url":"#week-01","title":"Week 01","items":[{"url":"#september-01--overview--logistics","title":"September 01 — Overview & Logistics"},{"url":"#september-04--basic-math","title":"September 04 — Basic Math"}]},{"url":"#week-02","title":"Week 02","items":[{"url":"#september-08--introduction-to-reinforcement-learning","title":"September 08 — Introduction to Reinforcement Learning"},{"url":"#september-11--multi-armed-bandits","title":"September 11 — Multi-Armed Bandits"}]},{"url":"#week-03","title":"Week 03","items":[{"url":"#september-15--markov-process","title":"September 15 — Markov Process"},{"url":"#september-18--dynamic-programming","title":"September 18 — Dynamic Programming"}]},{"url":"#week-04","title":"Week 04","items":[{"url":"#september-22--tutorial-on-gymnasium","title":"September 22 — Tutorial on Gymnasium"},{"url":"#september-25--monte-carlo-methods-on-policy-methods","title":"September 25 — Monte-Carlo Methods: On-Policy Methods"}]},{"url":"#week-05","title":"Week 05","items":[{"url":"#september-29--monte-carlo-methods-off-policy-methods","title":"September 29 — Monte-Carlo Methods: Off-Policy Methods"},{"url":"#october-02--temporal-difference-learning","title":"October 02 — Temporal Difference Learning"}]},{"url":"#week-06","title":"Week 06","items":[{"url":"#october-06--chuseok-holiday","title":"October 06 — Chuseok Holiday"},{"url":"#october-09--hangul-day","title":"October 09 — Hangul day"}]},{"url":"#week-07","title":"Week 07","items":[{"url":"#october-13--competition-round-1-grid-crossing","title":"October 13 — Competition Round 1: Grid Crossing!"},{"url":"#october-16--n-step-bootstrapping","title":"October 16 — n-Step Bootstrapping"}]},{"url":"#week-08","title":"Week 08","items":[{"url":"#october-20--focus-on-midterm-exam","title":"October 20 — Focus on Midterm Exam"},{"url":"#october-23-planning--learning-model-based-methods--experience-sampling","title":"October 23— Planning & Learning: Model-based Methods & Experience Sampling"}]},{"url":"#week-09","title":"Week 09","items":[{"url":"#october-27--planning--learning-trajectory-sampling--decision-time-planning","title":"October 27 — Planning & Learning: Trajectory Sampling & Decision-Time Planning"},{"url":"#october-30--competition-round-2-grid-adventure","title":"October 30 — Competition Round 2: Grid Adventure!"}]},{"url":"#week-10","title":"Week 10","items":[{"url":"#november-03--function-approximation-basics--non-parametric-fa","title":"November 03 — Function Approximation: Basics & Non-Parametric FA"},{"url":"#november-06--function-approximation-linear-function-approximation","title":"November 06 — Function Approximation: Linear Function Approximation"}]},{"url":"#week-11","title":"Week 11","items":[{"url":"#november-10--function-approximation-artificial-neural-network","title":"November 10 — Function Approximation: Artificial Neural Network"},{"url":"#november-13--function-approximation-convolution-neural-network","title":"November 13 — Function Approximation: Convolution Neural Network"}]},{"url":"#week-12","title":"Week 12","items":[{"url":"#november-17--function-approximation-deep-q-network","title":"November 17 — Function Approximation: Deep-Q Network"},{"url":"#november-20--competition-round-3-avoid-shits","title":"November 20 — Competition Round 3: Avoid Shits!"}]},{"url":"#week-13","title":"Week 13","items":[{"url":"#november-24--policy-gradient-basics","title":"November 24 — Policy Gradient: Basics"},{"url":"#november-27--policy-gradient-reinforce--actor-critic-methods","title":"November 27 — Policy Gradient: REINFORCE & Actor-Critic Methods"}]},{"url":"#week-14","title":"Week 14","items":[{"url":"#december-01--policy-gradient-actor-critic-with-eligibility-traces--asynchronous-advantage-actor-critic","title":"December 01 — Policy Gradient: Actor-Critic with Eligibility Traces & Asynchronous Advantage Actor-Critic"},{"url":"#december-04---self-play","title":"December 04 - Self-Play"}]},{"url":"#week-15","title":"Week 15","items":[{"url":"#december-08--focus-on-final-exam","title":"December 08 — Focus on Final Exam"},{"url":"#december-11--competition-round-5-league-stage","title":"December 11 — Competition Round 5: League Stage"}]},{"url":"#week-16","title":"Week 16","items":[{"url":"#december-15--competition-round-5-playoff-stage","title":"December 15 — Competition Round 5: Playoff Stage"},{"url":"#december-18--final-remark","title":"December 18 — Final Remark"}]}]}]}}},"pageContext":{"id":"ac9c858d-dfae-52f4-a836-f149ad99c2d6","frontmatter":{"code":"4471032","year":"2025","semester":"fall","division":"2 Div.","title":"Artificial Intelligence","types":"major","description":"Reinforcement learning (RL) is one of the popular machine learning paradigms for solving sequential decision-making problems. In this paradigm, agents learn the optimal policies by repeatedly interacting with an environment to maximize (cumulative) rewards. This courses will cover the foundational concepts of RL, including state-action-reward pairs, the Markov decision process, and exploration versus exploitation. In addition, we will learn key RL algorithms, such as the Monte Carlo method, temporal difference learning, function approximation, and policy gradients. Furthermore, you will work on a small team project to implement an RL agent to solve problems with different difficulties, from simple to complex ones."}}},"staticQueryHashes":["3751811173","892254660"],"slicesMap":{}}